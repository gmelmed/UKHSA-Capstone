{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running prompts on OpenAI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0. BASE SETTING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _0.1 Setting_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markdown in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: annoy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.17.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.55.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (1.15.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp313-cp313-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install markdown\n",
    "%pip install annoy\n",
    "%pip install openai\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI # OpenAI API\n",
    "import json\n",
    "import requests # to download some resources\n",
    "import os # file operations\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "from markdown import markdown # to render markdown\n",
    "from IPython.display import Markdown\n",
    "import annoy # Approximate Nearest Neighbors Oh Yeah for fast searching\n",
    "import pickle\n",
    "from annoy import AnnoyIndex\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _0.2 Loading MD files of the sample of countries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE MD FILES\n",
    "\n",
    "sample_countries = ['Netherlands','Norway', 'Chile']\n",
    "\n",
    "# Folder with the MD files\n",
    "folder_mds = \"../data/3-naps-md\"\n",
    "\n",
    "# Importing MDs files\n",
    "for file in os.listdir(folder_mds):\n",
    "    for country in sample_countries:\n",
    "        if country in file:  \n",
    "            with open(os.path.join(folder_mds, file), \"r\", encoding=\"utf-8\") as md_file:\n",
    "                content = md_file.read()\n",
    "            globals()[country.lower()] = content # Saving the MD file in lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _0.3 Country to evaluate_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_test = netherlands\n",
    "country_name = \"netherlands\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. SETTING UP THE MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _1.1 API key_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our api key\n",
    "with open('../api-keys/our_api_key.txt', 'r') as file:\n",
    "    api_key = file.read().replace('\\n', '')\n",
    "\n",
    "# read in finns api key (the one we'll use for testing)\n",
    "with open('../api-keys/finns_api_key.txt', 'r') as file:\n",
    "    finns_api_key = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the OpenAI\n",
    "client = OpenAI(api_key=finns_api_key) # using finns for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _1.2 Chunks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: Chunk the markdown\n",
    "def chunk_markdown(md_text, max_chars=3000):\n",
    "    \"\"\"Chunks some markdown by adding new lines until exceeding max_chars.\n",
    "       Each chunk includes the last line of the previous chunk.\"\"\"\n",
    "\n",
    "    lines = md_text.split(\"\\n\")  # Split into lines\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        # Always include the previous line for context\n",
    "        if i > 0 and current_length + len(line) > max_chars:\n",
    "            chunks.append(\"\\n\".join(current_chunk))  # Save the current chunk\n",
    "            current_chunk = [lines[i-1]]  # Start new chunk with the preceding line\n",
    "            current_length = len(lines[i-1])  # Reset length tracker\n",
    "\n",
    "        current_chunk.append(line)\n",
    "        current_length += len(line) + 1  # +1 for the newline character\n",
    "\n",
    "    # Add the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(\"\\n\".join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking the markdown\n",
    "chunks = chunk_markdown(country_test, max_chars=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. RUNNING THE MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _2.1 Question 0: Period for NAP_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_period = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature= 0,\n",
    "        top_p = 0.5,\n",
    "        response_format={\"type\":\"json_object\"},\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "            You are an assistant that extract information from national action plans for antimicrobial resistance (AMR). \n",
    "            Be concise and rely only on the text content.\n",
    "\n",
    "            From this section of the National Action Plan, extract the period considered for the actions of the current National Action Plan and return it in a structured JSON format.\n",
    "            If the period is not mentioned, return the period as null. No include other periods related to specific actions or goals.\n",
    "            \n",
    "            The response **must** follow this exact JSON structure:\n",
    "                {\n",
    "                    \"period_start\": The year when starts the period as numeric or null,\n",
    "                    \"period_end\": The year when ends the period as numeric or null\n",
    "                    \"supporting_chunk\": If the period_start and period_end are not null, include the all chunk text that supports the answer\n",
    "                }\n",
    "            \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": chunk},\n",
    "        ]\n",
    "    )\n",
    "    response_content = response.choices[0].message.content\n",
    "    parsed_response = json.loads(response_content)\n",
    "    responses_period.append(parsed_response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'country': 'netherlands', 'question': 'period_start', 'answer': 2015}, {'country': 'netherlands', 'question': 'period_end', 'answer': 'null'}]\n"
     ]
    }
   ],
   "source": [
    "start_years = [r[\"period_start\"] for r in responses_period if r[\"period_start\"] != None]\n",
    "end_years = [r[\"period_end\"] for r in responses_period if r[\"period_end\"] != None]\n",
    "\n",
    "# Function to find the most recurrent year\n",
    "def most_recurrent_year(years):\n",
    "    year_freq = {}\n",
    "    \n",
    "    # Count the frequency of each year\n",
    "    for year in years:\n",
    "        if year in year_freq:\n",
    "            year_freq[year] += 1\n",
    "        else:\n",
    "            year_freq[year] = 1\n",
    "    \n",
    "    # Find the year with the maximum frequency\n",
    "    if year_freq:\n",
    "        return max(year_freq, key=year_freq.get)\n",
    "    return \"null\"\n",
    "\n",
    "# Final result using the most recurrent year\n",
    "final_response_period = [\n",
    "    {\n",
    "        \"country\": country_name,\n",
    "        \"question\": \"period_start\",\n",
    "        \"answer\": most_recurrent_year(start_years)\n",
    "    },\n",
    "    {\n",
    "        \"country\": country_name,\n",
    "        \"question\": \"period_end\",\n",
    "        \"answer\": most_recurrent_year(end_years)\n",
    "    }\n",
    "]\n",
    "\n",
    "# Printing the final result\n",
    "print(final_response_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _2.2 Questions 1-19: Yes/No_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTIONS\n",
    "questions_yesno = [\n",
    "    \"Does the NAP include any strategy/policy/goal/action related to the priority sector of 'Human Health'?\",\n",
    "    \"Does the NAP include any strategy/policy/goal/action related to the priority sector of 'Animal Health'?\",\n",
    "    \"Does the NAP include any strategy/policy/goal/action related to the priority sector of 'Environment'?\",\n",
    "    \"Does the NAP include any strategy/policy/goal/action related to the priority sector of 'Agriculture/Food Security'?\",\n",
    "    \"Does the NAP include any mechanisms for progress reporting to track how its objectives are being met, such as an annual or semi-annual progress report, a dashboard displaying the status, or similar documents/tools?\",\n",
    "    \"Does the NAP include any specific, time-bound targets to track progress?\",\n",
    "    \"Does the NAP specify any budget allocation assigned for the strategies/policies/goals/actions directed at tackling AMR?\",\n",
    "    \"Does the NAP establish a multisectoral committee or task force for AMR coordination?\",\n",
    "    \"Does the NAP include any strategy/policy/goal/action related to training and educational initiative to increase awareness about AMR in school curricula or professional training programs for doctors and pharmacists?\",\n",
    "    \"Does the NAP include any strategy/policy/goal/action related to public awareness campaigns in combating AMR, such as media involvement and community engagement?\",\n",
    "    \"Does the NAP include any strategy/policy/goal/action related to international collaboration and support for AMR initiatives, such as partnerships with organizations like World Health Organization (WHO), Food and Agriculture Organization (FAO), and World Organisation for Animal Health (OIE)?\",\n",
    "    \"Does the NAP include any strategy/policy/goal/action related to data collection and analysis for AMR surveillance, such as developing surveillance systems, data collection tools, setting up laboratories for AMR monitoring, or other similar actions?\",\n",
    "    \"Does the NAP include any strategy/policy/goal/action related to developing the capacity to detect and report newly emerged resistance that may constitute a public health emergency of international concern, as required by the International Health Regulations (2005)?\",\n",
    "    \"Does the NAP include any strategy/policy/goal/action related to specific prevention strategies such as sanitation, hygiene measures, and infection control?\",\n",
    "    \"Does the NAP include any strategy/policy/goal/action related to implementing the standards outlined in the OIE Terrestrial and Aquatic Animal Health Codes or the World Health Organization's/Food and Agriculture Organization's Codex Alimentarius Code of Practice to Minimize and Contain Antimicrobial Resistance?\",\n",
    "    \"Does the NAP include any strategy/policy/goal/action related to regulating the distribution, prescription, and dispensation of antibiotics, such as developing or maintaining essential medicine lists for antibiotics?\",\n",
    "    \"Does the NAP include any strategy/policy/goal/action related to strengthening the legislative and regulatory framework for AMR, including laws and regulations for antimicrobial use?\",\n",
    "    \"Does the NAP include any strategy/policy/goal/action related to requiring the rational use of antibiotics through regulations/laws?\",\n",
    "    \"Does the NAP include any strategy/policy/goal/action related to participating in international collaborative research to support the development of new medicines, diagnostic tools, and vaccines?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_yesno = []\n",
    "\n",
    "for j, question in enumerate(questions_yesno):\n",
    "    final_answer = \"no\" # Default answer\n",
    "    supporting_chunks = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature= 0,\n",
    "            top_p = 0.5,\n",
    "            response_format={\"type\":\"json_object\"},\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"\"\"\n",
    "                You are an assistant that analyze and summarise information from national action plans for antimicrobial resistance (AMR). \n",
    "                Be concise and rely only on the text content.\n",
    "\n",
    "                From this section of the National Action Plan, analyze and answer the following yes/no question: \"{question}\".\n",
    "                If you cannot answer with the information provided, return null.\n",
    "                \n",
    "                The response **must** follow this exact JSON structure:\n",
    "                {{\n",
    "                    \"value\": yes/no/null,\n",
    "                    \"supporting_chunk\": \"If the answer is yes, include the chunk text that supports the answer\"\n",
    "                }}\n",
    "                \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": chunk},\n",
    "            ]\n",
    "        )\n",
    "        response_content = response.choices[0].message.content\n",
    "        parsed_response = json.loads(response_content)\n",
    "        value = parsed_response.get(\"value\")\n",
    "        supporting_chunk = parsed_response.get(\"supporting_chunk\")    \n",
    "        if value == \"yes\":\n",
    "            final_answer = \"yes\"\n",
    "            supporting_chunks.append(supporting_chunk)            \n",
    "        \n",
    "    # After checking all chunks for this question, append the final result\n",
    "    number_question = j + 1\n",
    "    responses_yesno.append({\n",
    "        \"country\": country_name,\n",
    "        \"question\": number_question,\n",
    "        \"answer\": final_answer,\n",
    "        \"supporting_chunks\": supporting_chunks if final_answer == \"yes\" else []\n",
    "    })  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 1\n",
      "Final Answer: no\n",
      "\n",
      "Question: 2\n",
      "Final Answer: no\n",
      "\n",
      "Question: 3\n",
      "Final Answer: no\n",
      "\n",
      "Question: 4\n",
      "Final Answer: no\n",
      "\n",
      "Question: 5\n",
      "Final Answer: no\n",
      "\n",
      "Question: 6\n",
      "Final Answer: no\n",
      "\n",
      "Question: 7\n",
      "Final Answer: no\n",
      "\n",
      "Question: 8\n",
      "Final Answer: no\n",
      "\n",
      "Question: 9\n",
      "Final Answer: no\n",
      "\n",
      "Question: 10\n",
      "Final Answer: no\n",
      "\n",
      "Question: 11\n",
      "Final Answer: no\n",
      "\n",
      "Question: 12\n",
      "Final Answer: no\n",
      "\n",
      "Question: 13\n",
      "Final Answer: no\n",
      "\n",
      "Question: 14\n",
      "Final Answer: no\n",
      "\n",
      "Question: 15\n",
      "Final Answer: no\n",
      "\n",
      "Question: 16\n",
      "Final Answer: no\n",
      "\n",
      "Question: 17\n",
      "Final Answer: no\n",
      "\n",
      "Question: 18\n",
      "Final Answer: no\n",
      "\n",
      "Question: 19\n",
      "Final Answer: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the final responses\n",
    "for response in responses_yesno:\n",
    "    print(f\"Question: {response['question']}\")\n",
    "    print(f\"Final Answer: {response['answer']}\")\n",
    "    if response['answer'] == \"yes\":\n",
    "        print(\"Supporting Chunks:\")\n",
    "        for chunk in response['supporting_chunks']:\n",
    "            print(chunk)\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining responses_yesno to the response_period\n",
    "summary_response = final_response_period.copy()\n",
    "summary_response.extend(responses_yesno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to a JSON string\n",
    "summary_response_json = json.dumps(summary_response, indent=4)\n",
    "\n",
    "# Saving\n",
    "with open(f'../data/4-summary-responses-json/summary_response_{country_name}_usingchunks.json', 'w') as f:\n",
    "    f.write(summary_response_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. CALCULATING THE PERFORMANCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Country to evaluate: Chile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for re-calculating the metrics with new annotations\n",
    "country_test = chile\n",
    "country_name = \"chile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_gs</th>\n",
       "      <th>comment_input_for_protocol</th>\n",
       "      <th>supporting_text_when_true_answer_is_yes_but_prediction_is_no</th>\n",
       "      <th>answer_llm</th>\n",
       "      <th>supporting_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chile</td>\n",
       "      <td>period_start</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chile</td>\n",
       "      <td>period_end</td>\n",
       "      <td>2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chile</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>[Generar instancias de capacitación locales pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chile</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>[111.6. Prevenir y controlar infecciones en la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chile</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>[111.5. Incorporar medidas regulatorias en rel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country      question answer_gs comment_input_for_protocol  \\\n",
       "0   chile  period_start      2021                        NaN   \n",
       "1   chile    period_end      2025                        NaN   \n",
       "2   chile             1       yes                        NaN   \n",
       "3   chile             2       yes                        NaN   \n",
       "4   chile             3       yes                        NaN   \n",
       "\n",
       "  supporting_text_when_true_answer_is_yes_but_prediction_is_no answer_llm  \\\n",
       "0                                                NaN                 2021   \n",
       "1                                                NaN                 2025   \n",
       "2                                                NaN                  yes   \n",
       "3                                                NaN                  yes   \n",
       "4                                                NaN                  yes   \n",
       "\n",
       "                                   supporting_chunks  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2  [Generar instancias de capacitación locales pa...  \n",
       "3  [111.6. Prevenir y controlar infecciones en la...  \n",
       "4  [111.5. Incorporar medidas regulatorias en rel...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the gold standard\n",
    "annotations_df = pd.read_excel('../data/5-annotations/annotations.xlsx')\n",
    "annotations_df = annotations_df[annotations_df['country'] == country_name]\n",
    "\n",
    "# Loading the LLM responses\n",
    "with open(f'../data/4-summary-responses-json/summary_response_{country_name}_usingchunks.json', 'r') as f:\n",
    "    llm_response = json.load(f)\n",
    "llm_response_df = pd.DataFrame(llm_response)\n",
    "#llm_response_df['question'] = llm_response_df['question'].astype(str)\n",
    "\n",
    "# Merge the ground truth with the LLM responses based on both country and question\n",
    "merged_df = pd.merge(annotations_df, llm_response_df, on=[\"country\", \"question\"], suffixes=('_gs', '_llm'))\n",
    "\n",
    "# keep the country of interest\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _3.1 Questions: Yes/No_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9047619047619048\n",
      "Precision: 0.875\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9333333333333333\n",
      "Questions where the model failed: [11 19]\n"
     ]
    }
   ],
   "source": [
    "# Calculate performance metrics\n",
    "merged_df['gs_binary'] = merged_df['answer_gs'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "merged_df['llm_binary'] = merged_df['answer_llm'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, and F1 Score\n",
    "accuracy = accuracy_score(merged_df['gs_binary'], merged_df['llm_binary'])\n",
    "precision = precision_score(merged_df['gs_binary'], merged_df['llm_binary'], zero_division=0)\n",
    "recall = recall_score(merged_df['gs_binary'], merged_df['llm_binary'], zero_division=0)\n",
    "f1 = f1_score(merged_df['gs_binary'], merged_df['llm_binary'], zero_division=0)\n",
    "questions_failed = merged_df[merged_df['gs_binary'] != merged_df['llm_binary']]['question'].unique()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Questions where the model failed: {questions_failed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _3.2 Questions: Extraction (period) ... IN PROGRESS_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Country to evaluate: Netherlands_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for re-calculating the metrics with new annotations\n",
    "country_test = netherlands\n",
    "country_name = \"netherlands\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_gs</th>\n",
       "      <th>comment_input_for_protocol</th>\n",
       "      <th>supporting_text_when_true_answer_is_yes_but_prediction_is_no</th>\n",
       "      <th>answer_llm</th>\n",
       "      <th>supporting_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>netherlands</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Actors agree to the following goals: The numbe...</td>\n",
       "      <td>no</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>netherlands</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The reduction of antibiotic use has flattened....</td>\n",
       "      <td>no</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>netherlands</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Despite these uncertainties, the research by Z...</td>\n",
       "      <td>no</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>netherlands</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>netherlands</td>\n",
       "      <td>5</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We will regularly inform Parliament of the pro...</td>\n",
       "      <td>no</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country question answer_gs comment_input_for_protocol  \\\n",
       "0  netherlands        1       yes                        NaN   \n",
       "1  netherlands        2       yes                        NaN   \n",
       "2  netherlands        3       yes                        NaN   \n",
       "3  netherlands        4        no                        NaN   \n",
       "4  netherlands        5       yes                        NaN   \n",
       "\n",
       "  supporting_text_when_true_answer_is_yes_but_prediction_is_no answer_llm  \\\n",
       "0  Actors agree to the following goals: The numbe...                   no   \n",
       "1  The reduction of antibiotic use has flattened....                   no   \n",
       "2  Despite these uncertainties, the research by Z...                   no   \n",
       "3                                                NaN                   no   \n",
       "4  We will regularly inform Parliament of the pro...                   no   \n",
       "\n",
       "  supporting_chunks  \n",
       "0                []  \n",
       "1                []  \n",
       "2                []  \n",
       "3                []  \n",
       "4                []  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the gold standard\n",
    "annotations_df = pd.read_excel('../data/5-annotations/annotations.xlsx')\n",
    "annotations_df = annotations_df[annotations_df['country'] == country_name]\n",
    "\n",
    "# Loading the LLM responses\n",
    "with open(f'../data/4-summary-responses-json/summary_response_{country_name}_usingchunks.json', 'r') as f:\n",
    "    llm_response = json.load(f)\n",
    "llm_response_df = pd.DataFrame(llm_response)\n",
    "#llm_response_df['question'] = llm_response_df['question'].astype(str)\n",
    "\n",
    "# Merge the ground truth with the LLM responses based on both country and question\n",
    "merged_df = pd.merge(annotations_df, llm_response_df, on=[\"country\", \"question\"], suffixes=('_gs', '_llm'))\n",
    "\n",
    "# keep the country of interest\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _3.1 Questions: Yes/No_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.42105263157894735\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Questions where the model failed: [1 2 3 5 6 9 10 11 14 16 19]\n"
     ]
    }
   ],
   "source": [
    "# Calculate performance metrics\n",
    "merged_df['gs_binary'] = merged_df['answer_gs'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "merged_df['llm_binary'] = merged_df['answer_llm'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, and F1 Score\n",
    "accuracy = accuracy_score(merged_df['gs_binary'], merged_df['llm_binary'])\n",
    "precision = precision_score(merged_df['gs_binary'], merged_df['llm_binary'], zero_division=0)\n",
    "recall = recall_score(merged_df['gs_binary'], merged_df['llm_binary'], zero_division=0)\n",
    "f1 = f1_score(merged_df['gs_binary'], merged_df['llm_binary'], zero_division=0)\n",
    "questions_failed = merged_df[merged_df['gs_binary'] != merged_df['llm_binary']]['question'].unique()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Questions where the model failed: {questions_failed}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
